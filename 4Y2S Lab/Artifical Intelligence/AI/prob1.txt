import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Function to generate the polynomial
def polynomial(x):
    return 7*x**3 - 12*x**2 - 22*x + 7

# Generate samples
x = np.linspace(-20, 20, 10000)
y = polynomial(x)

# Normalize the data to the range -1 to 1
scaler = MinMaxScaler(feature_range=(-1, 1))
x = scaler.fit_transform(x.reshape(-1, 1))
y = scaler.fit_transform(y.reshape(-1, 1))

# Split the data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=42)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.053, random_state=42)  # 0.053 is approximately 5% of 0.95

# Build the model
model = Sequential([
    Dense(32, input_shape=(1,), activation='relu'),
    Dense(64, activation='relu'),
    Dense(128, activation='relu'),
    Dense(1, activation='linear')
])

model.summary()  # This prints the details of the model, including number of parameters

# Compile and train the model
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val))

# Plotting training and validation accuracy
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()

# Make predictions
y_pred = model.predict(x_test)

# Rescale to original range for plotting true vs predicted
x_test = scaler.inverse_transform(x_test)
y_test = scaler.inverse_transform(y_test)
y_pred = scaler.inverse_transform(y_pred)

# Plot the prediction accuracy vs true levels
plt.subplot(1, 2, 2)
plt.plot(x_test, y_test, label='True')
plt.plot(x_test, y_pred, label='Predicted')
plt.legend()

plt.show()

